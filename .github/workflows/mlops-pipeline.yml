name: MLOps Pipeline

# Defines the events that trigger this workflow.
on:
  # In this case we will just run the pipeline when we add content to the repo
  push:
    branches: [ main , master ]
    # Runs also when a new tag following the pattern vX.Y.Z is pushed (e.g., v1.0.0).
    tags: [ 'v*.*.*' ] 
    
  # pull_request:
  #   branches: [ main ]

# ==============================================================================
# JOBS: Define the main stages of the MLOps pipeline
# ==============================================================================
jobs:
  # --- Stage 1: Data Preprocessing and Feature Engineering ---
  data-processing:
    runs-on: ubuntu-latest
    steps:
  
      # Downloads the repository code onto the runner.
      - name: Checkout code
        uses: actions/checkout@v2

      # Sets up the specified Python environment 
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.11.9'

      # Installs project dependencies from requirements.txt.
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # It takes the raw CSV and outputs a cleaned CSV.
      - name: Process data
        run: |
          python src/data/run_processing.py \
          --input data/raw/house_data.csv \
          --output data/processed/cleaned_house_data.csv

      # It takes the cleaned data, creates features, and saves the feature transformer (preprocessor).
      - name: Engineer features
        run: |
          python src/features/engineer.py \
          --input data/processed/cleaned_house_data.csv \
          --output data/processed/featured_house_data.csv \
          --preprocessor models/trained/preprocessor.pkl

      # Uploads the final processed data as an artifact, making it available for subsequent jobs (model-training).
      - name: Upload processed data
        uses: actions/upload-artifact@v4
        with:
          name: processed-data
          path: data/processed/featured_house_data.csv

      # Uploads the fitted scikit-learn preprocessor (transformer) as an artifact for later use in prediction/deployment.
      - name: Upload preprocessor
        uses: actions/upload-artifact@v4
        with:
          name: preprocessor
          path: models/trained/preprocessor.pkl

  # --- Stage 2: Model Training and Experiment Tracking (MLflow) ---
  model-training:
    # This job must wait for the 'data-processing' job to successfully complete.
    needs: data-processing
    runs-on: ubuntu-latest

    steps:
      # Downloads the repository
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.11.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Each job (e.g., data-processing and model-training) runs on a completely separate, fresh virtual machine (VM). 
      # so we need to download it
      - name: Download processed data
        uses: actions/download-artifact@v4
        with:
          name: processed-data
          path: data/processed/

      # Sets up a local MLflow tracking server using Docker.
      # This allows the training script to log experiments to a consistent location (http://localhost:5000).
      - name: pull the image of MLflow
        # we pull the image from image from GitHub Container Registry (GHCR), its official for MLflow
        run: |
          docker pull ghcr.io/mlflow/mlflow:latest

      - name: Set up MLflow
        # setup MLflow
        run: |
          docker run -d -p 5000:5000 --name mlflow-server \
          ghcr.io/mlflow/mlflow:latest mlflow server \
          --host 0.0.0.0 --backend-store-uri sqlite:///mlflow.db

      # Polls the MLflow server health endpoint, waiting for it to be fully operational before proceeding.
      - name: Wait for MLflow to start
        run: |
          for i in {1..10}; do
            curl -f http://localhost:5000/health || sleep 5;
          done

      # Executes the model training script.
      # The script loads data, trains the model, and logs metrics/parameters to the local MLflow server.
      - name: Train model
        run: |
          mkdir -p models
          python src/models/train_model.py \
          --config configs/model_config.yaml \
          --data data/processed/featured_house_data.csv \
          --models-dir models \
          --mlflow-tracking-uri http://localhost:5000

      # Uploads the final trained model file(s) as an artifact for the deployment stage.
      - name: Upload trained model
        uses: actions/upload-artifact@v4
        with:
          name: trained-model
          path: models/

      # Cleans up the MLflow Docker container after training to free up resources.
      - name: Clean up MLflow
        run: |
          docker stop mlflow-server || true
          docker rm mlflow-server || true

  # --- Stage 3: Docker Image Building and Publishing ---
  build-and-publish:
    # This job must wait for the 'model-training' job to successfully complete.
    needs: model-training
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      # Downloads the trained model artifact from the 'model-training' job.
      - name: Download trained model
        uses: actions/download-artifact@v4
        with:
          name: trained-model
          path: models/

      # Downloads the preprocessor artifact from the initial 'data-processing' job.
      # This is crucial for the deployment environment to handle incoming data correctly.
      - name: Download preprocessor
        uses: actions/download-artifact@v4
        with:
          name: preprocessor
          path: models/trained/

      # Sets up Docker Buildx for advanced Docker features (e.g., better caching).
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Authenticates with Docker Hub using repository secrets and variables to enable pushing the image.
      - name: Log in to DockerHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: docker.io
          username: ${{ vars.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
          
      # Builds the Docker image and pushes it to Docker Hub.
      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          # Sets the build context to the root directory (.).
          context: .
          # name of the file that run the container
          file: Dockerfile.fastapi
          # Instructs the action to push the built image.
          push: true
          # Defines the image tag structure: docker.io/USER/image_name:tag
          tags: docker.io/${{ vars.DOCKERHUB_USERNAME }}/house-price-model:latest
